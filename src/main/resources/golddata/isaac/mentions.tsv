13050adb7aa8aaf2e1c38f2b2c7e3d070358d261	Self-stabilization [1] is a general technique to design distributed systems that can tolerate arbitrary transient faults.|[1]	Numerous self-stabilizing solutions exist for this problem, e.g., [2,3,4].|2	Numerous self-stabilizing solutions exist for this problem, e.g., [2,3,4].|3	Numerous self-stabilizing solutions exist for this problem, e.g., [2,3,4].|4	Self-stabilizing solutions for the message forwarding problem are proposed in [5,6].|5	Self-stabilizing solutions for the message forwarding problem are proposed in [5,6].|6	Our goal is to provide a snap-stabilizing solution for this problem. A snap-stabilizing protocol [7] guarantees that, starting from any configuration, it always behaves according to its specification, i.e., it is a self-stabilizing algorithm which is optimal in terms of stabilization time since it stabilizes in 0 steps.|[7]	Note that the PIF algorithm introduced here is similar to the one proposed in [7] which is a snap stabilizing algorithm.|[7]	The first snap-stabilizing solution for this problem can be found in [8].|[8]	The number of buffers is reduced to D in [9], which improves the scalability aspect.|[9]	We consider in our work the classical local shared memory model introduced by Dijkstra [10] known as the state model.|[10]	This execution model is known as the distributed daemon [11].|[11]	In order to conceive our snap stabilizing algorithm we will use a structure called Buffer Graph introduced in [12].|[12]	In particular, by combining a snap-stabilizing message forwarding protocol with any self-stabilizing overlay protocols (e.g., [13] for DHT or [14,15,16] for tries), we would get a solution ensuring users to get right answers by querying the overlay architecture.|[13]	In particular, by combining a snap-stabilizing message forwarding protocol with any self-stabilizing overlay protocols (e.g., [13] for DHT or [14,15,16] for tries), we would get a solution ensuring users to get right answers by querying the overlay architecture.|[14]	In particular, by combining a snap-stabilizing message forwarding protocol with any self-stabilizing overlay protocols (e.g., [13] for DHT or [14,15,16] for tries), we would get a solution ensuring users to get right answers by querying the overlay architecture.|[15]	In particular, by combining a snap-stabilizing message forwarding protocol with any self-stabilizing overlay protocols (e.g., [13] for DHT or [14,15,16] for tries), we would get a solution ensuring users to get right answers by querying the overlay architecture.|[16]
0eb7343cdd90265282bd261c0e48cf2fd73ea465	We start with the biological neural network of a small living organism, a worm named Caenorhabditis elegans, which has been described in great details in [1,29].|1	All the connections between its neurons have been mapped [1,29] and are believed to be relatively well conserved between individual worms.|1	A part of this system, comprising 20 neurons and referred to as the ”pharyngeal system” is dedicated to control rhythmic contractions of a muscular pump that sucks food into the worm body [1].|[1]	Not long after, this result was contradicted by an article from H.E. Stanley’s team that studied outgoing and incoming connectivity separately (and ignored gap junctions) and showed that both distributions were exponential, thus excluding scale-free properties [2].|[2]	A first study claimed the distribution was compatible with a power-law (graphs determined by power-law distributions are also called “scale-free” graphs) [3].|[3]	For instance, most scale-free networks are obtained through a preferential attachment rule which postulates that new nodes are linked to the already most connected nodes [3].|[3]	Most complex networks can be categorized into four families [4].|[4]	Hence the same (or similar) reasonings can be applied to understand cell metabolism [17], the citation of scientific articles [22], software architecture [27], the Internet [3] or electronic circuits [5].|[5]	Moreover, wiring length optimization seems to be a crucial factor of cortical circuit development [6,7].|6	Moreover, wiring length optimization seems to be a crucial factor of cortical circuit development [6,7].|7	Long distance connections are expensive in biological neural networks because they imply large volumes of metabolically active tissue to be maintained and long transmission delays [8], just like links between two nodes in Internet or in airport transportation systems are more costly.|[8]	Considering the network is treated here as a directed graph, these results are coherent with previously published estimates [28,9].|9	An improved variation of the Watts-Strogatz algorithm restricts rewiring to a local spatial neighborhood around each node [9] thus implicitly introducing the distance factor.|[9]	Note that such models had not yet been derived by and are not directly useful to biologists: there are many studies on biological neural networks, but they focus on the identification of regular biological networks with clear structures, such as the basic circuit of the visual cortex [10], and they do not account for the seemingly irregular structure of the vast majority of biological neural networks.|[10]	At a much coarser grain, graph theory methods have recently been applied to networks of cortical areas [26,11],3 i.e., not networks of neurons but networks of neuron areas, with the prospect of understanding the network functions.|11	While proposing computing structures based on biological neurons may seem preposterous at first sight, G. Zeck and P. Fromherz [12,30] at the Max Planck Institute for Biochemistry in Martinsried, Germany, have recently demonstrated they can interface standard silicon chips with biological neurons, pass electrical signals back and forth through one or several biological neurons, much like we intend to do with carbon nanotubes, i.e., hybrid carbon nanotures/standard CMOS chips [13].|12	Finally, through this combined model, we will investigate the application of such biological neural networks to computing tasks, assuming the experimental setups described in [12].|[12]	While proposing computing structures based on biological neurons may seem preposterous at first sight, G. Zeck and P. Fromherz [12,30] at the Max Planck Institute for Biochemistry in Martinsried, Germany, have recently demonstrated they can interface standard silicon chips with biological neurons, pass electrical signals back and forth through one or several biological neurons, much like we intend to do with carbon nanotubes, i.e., hybrid carbon nanotures/standard CMOS chips [13].|[13]	The vast literature on artificial neural networks provides little indications on the structures of biological neural networks [14].|[14]	For instance, small-world properties (as well as, under some circumstances, scale-free connectivity [15]) have been shown to emerge naturally upon minimization of the euclidian distance between nodes [20].|[15]	Moreover, based on this research work, Infineon (one of the main European chip manufacturers) has recently announced it is investigating a prototype of a chip (called ”NeuroChip”) that can interconnect a grid of transistors with a network of biological neurons [16], based on Fromherz’s research work.|[16]	Hence the same (or similar) reasonings can be applied to understand cell metabolism [17], the citation of scientific articles [22], software architecture [27], the Internet [3] or electronic circuits [5].|[17]	Furthermore, Kaiser et al. [18] have recently shown that network structure during growth in a metric space is influenced by neuron density (number of neurons per unit volume) when growth occurs in a spatially constrained domain.|[18]	If one sets Pnew = 1, i.e., a new neuron is certain to be created in a currently empty location, our algorithm is essentially a three-dimensional extension of the model recently proposed by Kaiser and Hilgetag [18].|[18]	Most network growth models do not consider this parameter [19].|[19]	For instance, small-world properties (as well as, under some circumstances, scale-free connectivity [15]) have been shown to emerge naturally upon minimization of the euclidian distance between nodes [20].|[20]	Following Morita et al. [21], we neglected here the pharyngeal system and only deal with the remaining 282 neurons.|[21]	Unlike Morita et al. [21], we treated each link as directed, i.e., we differentiated links from neuron i to neuron j and links from j to i; however, we collapsed multiple identical links into a single one.|[21]	Finally, Morita et al. put forward correlations among incoming, outgoing and gap junctions to explain that the total degree (incoming + outgoing + gap junctions) was neither exponential nor displayed a clear power law decrease [21].|[21]	Hence the same (or similar) reasonings can be applied to understand cell metabolism [17], the citation of scientific articles [22], software architecture [27], the Internet [3] or electronic circuits [5].|[22]	Based on this work, Oshio et al. [23] have recently built a database which describes this biological neural network and facilitates its manipulation.|[23]	To construct a graph model of this system, we used the electronic database recently published by Oshio et al. [23].|[23]	Interestingly, recent results in neurobiology suggest that the lack of neural turnover and/or replacement of injured neurons in the adult brain is not due to the absence of potentially competent cell, but, more probably, to a strong reluctance of the neurons to accept newcomers into an already established neural network [24].|[24]	Besides the number of nodes N and number of links K, the structural characteristics of complex networks are mainly quantified by their link density ρ, average connectivity hki, connectivity distribution P(k), average shortest path λ and average clustering coefficient hCi [25].|[25]	At a much coarser grain, graph theory methods have recently been applied to networks of cortical areas [26,11],3 i.e., not networks of neurons but networks of neuron areas, with the prospect of understanding the network functions.|26	Hence the same (or similar) reasonings can be applied to understand cell metabolism [17], the citation of scientific articles [22], software architecture [27], the Internet [3] or electronic circuits [5].|[27]	Considering the network is treated here as a directed graph, these results are coherent with previously published estimates [28,9].|28	Similar arguments can be opposed to the Watts-Strogatz rewiring algorithm that generates small-world networks through addition of long-range connections to a pre-existing regular circular network [28].|[28]	Based on empirical data of a tiny organism, we have elaborated a model for biological neural networks. In agreement with previous works [28], we have found that the neural network of C. elegans has a graph structure with small-world properties, like many complex systems found in nature.|[28]	We start with the biological neural network of a small living organism, a worm named Caenorhabditis elegans, which has been described in great details in [1,29].|29	All the connections between its neurons have been mapped [1,29] and are believed to be relatively well conserved between individual worms.|29	While proposing computing structures based on biological neurons may seem preposterous at first sight, G. Zeck and P. Fromherz [12,30] at the Max Planck Institute for Biochemistry in Martinsried, Germany, have recently demonstrated they can interface standard silicon chips with biological neurons, pass electrical signals back and forth through one or several biological neurons, much like we intend to do with carbon nanotubes, i.e., hybrid carbon nanotures/standard CMOS chips [13].|30
fd906edf5833d0d506a220097ced14a03ff40a73	Analogous to PoA, strong price of anarchy (SPOA) (Andelman, Feldman, and Mansour 2009) in our case is the ratio of the largest average value to the average value of the winning consortium in the worst strong equilibrium.|Andelman, Feldman, and Mansour 2009	A better notion is a strong (Nash) equilibrium, which requires that no subset of the players can jointly deviate and increase each of their utilities (Aumann 1999).|Aumann 1999	While most works in classic cooperative game theory are only remotely related to our specific model, a series of papers on the stability of coalition structures, see for example (Demange 1994), (Bogomolnaia and Jackson 2002), and references therein, is quite relevant to our work.|Bogomolnaia and Jackson 2002	Probably, the most related paper to ours is (Chalkiadakis et al. 2009).|Chalkiadakis et al. 2009	While most works in classic cooperative game theory are only remotely related to our specific model, a series of papers on the stability of coalition structures, see for example (Demange 1994), (Bogomolnaia and Jackson 2002), and references therein, is quite relevant to our work.|Demange 1994	Our paper is also related to the literature on network creation games, starting with (Fabrikant et al. 2003).|Fabrikant et al. 2003	(This seems more related than the famous stable marriage problem (Gusfield and Irving 1989).)|Gusfield and Irving 1989	(Konishi, Le Breton, and Weber 1997) models the creation of coalitions in a game that bears similarities to our initial example game (the gold-rush).|Konishi, Le Breton, and Weber 1997	It has become standard in the algorithmic game theory literature to measure the quality of a game/protocol by its price of anarchy (POA) (Koutsoupias and Papadimitriou 1999).|Koutsoupias and Papadimitriou 1999	(Proofs in this section are given in the full version of this paper (Kutten, Lavi, and Trehan 2013).)|Kutten, Lavi, and Trehan 2013	(Myerson 1977) studies cooperative games over graphs, where only connected coalitions S are able to extract their value v(S).|Myerson 1977	In this work we introduce three natural protocols/games for deciding the composition of groups, and study their price of anarchy (POA): (Nisan et al. 2007) the ratio between (a) the optimal (maximal) average quality of an eligible set of researchers, and (b) the lowest average quality of a winner set that can be formed in an equilibrium (here we use both the notions of a Nash equilibrium and of a strong equilibrium).|Nisan et al. 2007	The literature on price of anarchy is rich, see (Nisan et al. 2007) for a survey.|Nisan et al. 2007
15aa277b1054cdcdf7fc018e3a3abe2df7a1691b	Predictive representations of state are a class of generative models that represent a dynamical system in terms of a set of predictions about sequences of observations generated by that system (Littman et al., 2002).|Littman et al., 2002	There is a large body of recent work on basis function selection and construction for value function approximation in Markov decision processes (Mahadevan, 2008; Parr et al., 2007), and it may be interesting to consider applying work in those areas to choosing or constructing appropriate features for observation spaces in partially observable, continuous domains.|Mahadevan, 2008	An online discovery algorithm for discrete TD networks was presented in Makino and Tagaki (2008).|Makino and Tagaki 2008	There is a large body of recent work on basis function selection and construction for value function approximation in Markov decision processes (Mahadevan, 2008; Parr et al., 2007), and it may be interesting to consider applying work in those areas to choosing or constructing appropriate features for observation spaces in partially observable, continuous domains.|Parr et al., 2007	Empirically it has also been shown that in certain domains predictive representations can lead to better generalization than other representations (Rafols et al., 2005).|Rafols et al., 2005	Recent work has shown that certain formalizations of predictive representations are strictly more expressive than other models of discrete dynamical systems that use historical information or probabilistic distributions over unobservable variables as a representation (e.g., k-Markov models, POMDPs) (Singh & James, 2004).|Singh & James,  2004	Eligibility traces were originally introduced in Sutton (1988) to provide a mechanism for making more general n-step backups of predictions in conventional TD learning, rather than the traditional 1-step backups.|Sutton 1988	Predictions of the values of these functions, which together define state, can then be used as features for approximating other functions, e.g, value functions in a reinforcement learning setting (Sutton & Barto, 1998).|Sutton & Barto, 1998	We refer the reader to Sutton and Barto (1998) for the details of the dynamics.|Sutton and Barto 1998	However, although temporal abstraction in discrete TD networks has been explored recently (Sutton et al., 2006), to our knowledge there has been no work on state abstraction in TD networks.|Sutton et al., 2006	One formalism for predictive representations is the Temporal-difference (TD) network (Sutton & Tanner, 2005).|Sutton & Tanner, 2005	Tanner and Sutton (2005) introduced TD(λ) networks, which incorporate eligibility traces to deal with certain shortcomings of conventional TD networks.|Tanner and Sutton 2005	We have only presented essential notation and intuition here, and refer the reader to Tanner and Sutton (2005) for the full details of the algorithm. In the following section we present our modified TD(λ) algorithm, which allows for continuous observations and actions.|Tanner and Sutton 2005	Modified from Tanner and Sutton (2005).|Tanner and Sutton 2005	The algorithm is modified from Tanner and Sutton (2005).|Tanner  and  Sutton 2005	This structure is similar to previous question network structures used in some discrete TD networks (Tanner & Sutton, 2005).|Tanner & Sutton, 2005	This is essentially a noisy, continuous analog of the cycle world presented in Tanner and Sutton (2005).|Tanner and Sutton 2005	We see that the network is able to learn a good model even given noisy observations with an amount of experience roughly equivalent to the amount taken to learn the deterministic, discrete version of this problem, as presented in Tanner and Sutton (2005).|Tanner and Sutton 2005	Although there has been some work on other formalisms of predictive representations in continuous systems (Wingate, 2008), these approaches have not yet been extended to a fully online, incremental setting.|Wingate, 2008	State abstraction in other predictive representation formalisms has been considered for both discrete (Wolfe et al., 2008) and continuous dynamical systems (Wingate, 2008).|Wingate, 2008	State abstraction in other predictive representation formalisms has been considered for both discrete (Wolfe et al., 2008) and continuous dynamical systems (Wingate, 2008).|Wolfe  et  al.,  2008
6ee7d70f2dbfc0d45fbf20485f82a9ed7e175725	Convolutional neural nets (convnets) trained from massive labeled datasets [1] have substantially improved the state-of-the-art in image classification [2] and object detection [3].|[1]	We perform experiments using a network architecture almost identical1 to that popularized by Krizhevsky et al. [2] and trained for classification using the 1.2 million images of the ILSVRC 2012 challenge dataset [1].|[1]	Convolutional neural nets (convnets) trained from massive labeled datasets [1] have substantially improved the state-of-the-art in image classification [2] and object detection [3].|[2]	Recent advances in convolutional neural nets [2] dramatically improved the state-of-the-art in image classification.|[2]	Convolutional neural networks have gained much recent attention due to their success in image classification [2].|[2]	We perform experiments using a network architecture almost identical1 to that popularized by Krizhevsky et al. [2] and trained for classification using the 1.2 million images of the ILSVRC 2012 challenge dataset [1].|[2]	Convolutional neural nets (convnets) trained from massive labeled datasets [1] have substantially improved the state-of-the-art in image classification [2] and object detection [3].|[3]	For coarse localization, such doubts were alleviated by record breaking results extending the same features to detection on PASCAL [3].|[3]	The feature representations learned from large data sets have been found to generalize well to other image classification tasks [20] and even to object detection [3, 21].|3	Inspired in part by [3, 34, 23], we train sliding window part detectors to predict keypoint locations independently.|3	R-CNN [3] and OverFeat [34] have both demonstrated the effectiveness of deep convolutional networks on the generic object detection task.|[3]	We present evidence that convnet features localize at a much finer scale than their receptive field sizes, that they can be used to perform intraclass alignment as well as conventional hand-engineered features, and that they outperform conventional features in keypoint prediction on objects from PASCAL VOC 2011 [4].|[4]	For this task we use keypoint data [15] on the twenty classes of PASCAL VOC 2011 [4].|[4]	Despite the magnitude of these results, many doubted [5] that the resulting features had the spatial specificity necessary for localization; after all, whole image classification can rely on context cues and overly large pooling regions to get the job done.|[5]	Congealing [6] is an unsupervised joint alignment method based on an entropy objective.|[6]	Deep congealing [7] builds on this idea by replacing hand-engineered features with unsupervised feature learning from multiple resolutions.|[7]	Inspired by optical flow, SIFT flow [8] matches densely sampled SIFT features for correspondence and has been applied to motion prediction and motion transfer.|[8]	We approach this difficult task in the style of SIFT flow [8]: we retrieve near neighbors using a coarse similarity measure, and then compute dense correspondences on which we impose an MRF smoothness prior which finally allows all images to be warped into alignment.|[8]	(Unlike the L1 regularization originally used by SIFT flow [8], this formulation maintains rotational invariance of w.)|[8]	For each target image (left column), we show warped versions of five nearest neighbor images aligned with conv4 flow (first row), and warped versions aligned with SIFT flow [8] (second row).|[8]	In particular, fine-grained categorization, the subject of many recent works, depends strongly on part localization [9, 10].|9	In particular, fine-grained categorization, the subject of many recent works, depends strongly on part localization [9, 10].|10	Most of the existing works on part localization or keypoint prediction focus on either facial landmark localization [11] or human pose estimation.|[11]	Human pose estimation has been approached using tree structured methods to model the spatial relationships between parts [12, 13, 14], and also using poselets [15] as an intermediate step to localize human keypoints [16, 17].|12	Human pose estimation has been approached using tree structured methods to model the spatial relationships between parts [12, 13, 14], and also using poselets [15] as an intermediate step to localize human keypoints [16, 17].|13	Human pose estimation has been approached using tree structured methods to model the spatial relationships between parts [12, 13, 14], and also using poselets [15] as an intermediate step to localize human keypoints [16, 17].|14	Human pose estimation has been approached using tree structured methods to model the spatial relationships between parts [12, 13, 14], and also using poselets [15] as an intermediate step to localize human keypoints [16, 17].|[15]	For this task we use keypoint data [15] on the twenty classes of PASCAL VOC 2011 [4].|[15]	Human pose estimation has been approached using tree structured methods to model the spatial relationships between parts [12, 13, 14], and also using poselets [15] as an intermediate step to localize human keypoints [16, 17].|16	Human pose estimation has been approached using tree structured methods to model the spatial relationships between parts [12, 13, 14], and also using poselets [15] as an intermediate step to localize human keypoints [16, 17].|17	Convnets trained with backpropagation were initially succesful in digit recognition [18] and OCR [19].|[18]	Convnets trained with backpropagation were initially succesful in digit recognition [18] and OCR [19].|[19]	The feature representations learned from large data sets have been found to generalize well to other image classification tasks [20] and even to object detection [3, 21].|[20]	The feature representations learned from large data sets have been found to generalize well to other image classification tasks [20] and even to object detection [3, 21].|21	Recently, Toshev et al. [22] trained a cascade of regression-based convnets for human pose estimation and Jain et al. [23] combine a weak spatial model with deep learning methods.|[22]	Recently, Toshev et al. [22] trained a cascade of regression-based convnets for human pose estimation and Jain et al. [23] combine a weak spatial model with deep learning methods.|[23]	Inspired in part by [3, 34, 23], we train sliding window part detectors to predict keypoint locations independently.|23	Zeiler and Fergus [24] provide several heuristic visualizations suggesting coarse localization ability.|[24]	Szegedy et al. [25] show counterintuitive properties of the convnet representation, and suggest that individual feature channels may not be more semantically meaningful than other bases in feature space.|[25]	A concurrent work [26] compares convnet features with SIFT in a standard descriptor matching task.|[26]	All experiments are implemented using caffe [27], and our network is the publicly available caffe reference model.|[27]	In Figure 1, we perform a nonparametric reconstruction of images from features in the spirit of HOGgles [28].|[28]	Optimization is performed using belief propagation, with the techniques suggested in [29].|[29]	Message passing is performed efficiently using the squared Euclidean distance transform [30].|[30]	Given the alignment field w, we warp target to source using bivariate spline interpolation (implemented in SciPy [31]).|[31]	We assess correctness using mean PCK [32].|[32]	We extract features at each keypoint using SIFT [33] and using the column of each convnet layer whose rf center lies closest to the keypoint.|[33]	Inspired in part by [3, 34, 23], we train sliding window part detectors to predict keypoint locations independently.|34	R-CNN [3] and OverFeat [34] have both demonstrated the effectiveness of deep convolutional networks on the generic object detection task.|[34]	R-CNN starts from bottom-up region proposal [35], which tends to overlook the signal from small parts.|[35]	We also train using dense SIFT descriptors for comparison. We compute SIFT on a grid of stride eight and bin size of eight using VLFeat [36].|[36]
d3ba6b48f62e2fe1802efb46c3799362572eeb1d	If the peak rates of some flows are comparable with the capacity C of the server, the Gaussian approximation in the theorem of Bahadur-Rao [1] does not hold.|[1]	Then, using the continuous mapping theorem, the P-a.s. continuity of the projection operator f → fT ([2, Theorems 5.1 and 15.1]), and Proposition 5, we know that for all T ≥ 0, FT (XN (.)) ⇒ FT (Ap (.)) as N → ∞.|2	This problem has been addressed by many authors, see for instance [3,14,9,28,33,34].|3	In particular, Bonald et al. in [3] compare the workload distribution of general packet arrival processes with that of an M/G/1 queue with Poisson/MTU arrivals (i.e., when each packet has fixed size equal to MTU (Maximum Transmission Unit)).|[3]	Motivated by the Better than Poisson conjecture studied in [3], in this section we compare the asymptotic workload distribution generated by regulated flows with that of an M/G/1 queue.|[3]	As claimed in [3], we expect to analyze the performance of regulated flows inside a network via constructing proper M/G/1 queues for performance upper bounds.|[3]	They also consider the so-called many-sources asymptotic framework, which is a key regime when the peak rate of sources is small compared with the server capacity and there are many-sources; other results on many-sources asymptotics can be found in [4,8,10,26].|4	When the transmission capacity C is large, and in particular, C/π = O(N), we are in the regime of the many-sources asymptotics, which have been studied by many authors, see for instance [4,10,26].|4	In fact, authors in [4,10] have studied the connections between the many-sources asymptotic rate It0 (ct + b) and the large buffer asymptotic decay rate H(c), defined as in [4] H(c) def = − lim b→∞ 1 Nb log P{WN (0) ≥ Nb}.|4	In fact, authors in [4,10] have studied the connections between the many-sources asymptotic rate It0 (ct + b) and the large buffer asymptotic decay rate H(c), defined as in [4] H(c) def = − lim b→∞ 1 Nb log P{WN (0) ≥ Nb}.|[4]	In addition, the authors of [4,10] also indicated that Poisson arrivals do not gain scale economies, i.e., 1 b I p t0 (ct+b) = ξ.|4	A formalism to study the performance of a network supporting regulated flows, called network calculus, has been developed by Cruz [11,12] and more recently by Le Boudec [5] and Chang [7].|[5]	The authors of [19] observed that as the number of sources increases with fixed total load, the mean delay of regulated traffic tends to converge to that of an M/G/1 queue fed with a marked Poisson process of parameters associated with the (σ, ρ, π) values; see Cao et. al [6] for results in a more general setting.|[6]	We apply the same technique as in the paper by Cao and Ramanan [6].|[6]	Using Lemmas 2 and 3, and similar techniques as in [6, Section III], we can show that for all x > 0 and ε > 0, there exist finite TX(ε) and NX (ε) > 0 such that for T > TX (ε) and N > NX (ε), P sup t∈[T,∞) (X N (t) − Ct) > x! < ε/3.|6	A formalism to study the performance of a network supporting regulated flows, called network calculus, has been developed by Cruz [11,12] and more recently by Le Boudec [5] and Chang [7].|[7]	Chang et al. [8] also derived an upper bound of the tail distribution of the workload but via partitioning the busy period.|[8]	They also consider the so-called many-sources asymptotic framework, which is a key regime when the peak rate of sources is small compared with the server capacity and there are many-sources; other results on many-sources asymptotics can be found in [4,8,10,26].|8	See also [31] for generalizations and alternative proofs of the results in [8,22,23].|8	The principal difference with [32] is that in our approach, we explicitly take into account the many sources effect and determine the rate function for the source, which is extremal for the overflow asymptotics, rather than a priori first bounding the probability and then trying to make the bound small [8,32].|8	We also compare against simulation the other performance upper bounds obtained via the many sources asymptotic approach in Section 3 and the Bernoulli approach [8] which bounds the moment generating function of regulated flows by a Hoeffding inequality.|[8]	This problem has been addressed by many authors, see for instance [3,14,9,28,33,34].|9	They also consider the so-called many-sources asymptotic framework, which is a key regime when the peak rate of sources is small compared with the server capacity and there are many-sources; other results on many-sources asymptotics can be found in [4,8,10,26].|10	When the transmission capacity C is large, and in particular, C/π = O(N), we are in the regime of the many-sources asymptotics, which have been studied by many authors, see for instance [4,10,26].|10	In fact, authors in [4,10] have studied the connections between the many-sources asymptotic rate It0 (ct + b) and the large buffer asymptotic decay rate H(c), defined as in [4] H(c) def = − lim b→∞ 1 Nb log P{WN (0) ≥ Nb}.|10	In addition, the authors of [4,10] also indicated that Poisson arrivals do not gain scale economies, i.e., 1 b I p t0 (ct+b) = ξ.|10	A formalism to study the performance of a network supporting regulated flows, called network calculus, has been developed by Cruz [11,12] and more recently by Le Boudec [5] and Chang [7].|11	A formalism to study the performance of a network supporting regulated flows, called network calculus, has been developed by Cruz [11,12] and more recently by Le Boudec [5] and Chang [7].|12	The proof of [13, Proposition 9.2.IV] can then be readily adapted to give convergence of the superposed process to the corresponding Poisson process.|13	This problem has been addressed by many authors, see for instance [3,14,9,28,33,34].|14	As pointed out by the authors of [32] (see also [15]), these approaches can be regarded as applications of the Hoeffding bound given in [21] and they obtain the upper bounds via majorizing the queue length by a sum of independent processes and computing bounds based on these independent processes.|[15]	We can utilize the mean delay results for network design when the traffic is best effort while the results reported in this paper are better for tight quality of service constraints [16].|[16]	Such extensions are similarly discussed in papers by Mandjes and Kim [27] and Guibert and Simonian [17], where they assume a local convex behavior of a rate function (see Equation (3) below).|[17]	Besides the better-than-Poisson asymptotics for many regulated flows with their total load fixed, we have also studied in [18] that regulated flows are better-than-Poisson in the large buffer asymptotics.|[18]	The authors of [19] observed that as the number of sources increases with fixed total load, the mean delay of regulated traffic tends to converge to that of an M/G/1 queue fed with a marked Poisson process of parameters associated with the (σ, ρ, π) values; see Cao et. al [6] for results in a more general setting.|[19]	Both of these results complement those reported in [19] in that we can compute bounds for both the mean buffer occupancy as well as the asymptotic.|[19]	By using [20, Theorem 10.7], it follows that P   X M i=1 Nn Xi j=1 Wfi,j (0) ≥ x   ∼ Y M i=1 a Nni i !   X L j=1 κj (j − 1)!Γ(j, xξ)   , where we have used the incomplete Gamma function Γ(a, x) def = R ∞ x t a−1 e −tdt.|20	As pointed out by the authors of [32] (see also [15]), these approaches can be regarded as applications of the Hoeffding bound given in [21] and they obtain the upper bounds via majorizing the queue length by a sum of independent processes and computing bounds based on these independent processes.|[21]	These have focused on the overflow probability and/or delay distribution for an arbitrary number of input flows. Kesidis and Konstantopoulos [22,23] studied the problem via characterizing the extremal traffic shape, which maximizes the fraction of time when the buffer content is above a threshold b.|22	See also [31] for generalizations and alternative proofs of the results in [8,22,23].|22	As noted in [22,23], the worst case traffic pattern is not unique.|22	Comparison of many sources rate functions for homogeneous sources (i.e., M = 1) with parameters σ = 16, ρ = 1, C = N × 2, and π = 9 -K&K rate function is the rate function obtained by Kesidis and Konstantopoulos [22].|[22]	These have focused on the overflow probability and/or delay distribution for an arbitrary number of input flows. Kesidis and Konstantopoulos [22,23] studied the problem via characterizing the extremal traffic shape, which maximizes the fraction of time when the buffer content is above a threshold b.|23	See also [31] for generalizations and alternative proofs of the results in [8,22,23].|23	As noted in [22,23], the worst case traffic pattern is not unique.|23	The overflow probability of the corresponding M/G/1 queue can be computed via the inversion of its Laplace transform which has a well-known explicit form [24].|[24]	With the knowledge of Palm theory [25] and the ergodicity of (AN (t)), we know that P = CP(WN (0) ≥ b)/ρ = P(WN (0) ≥ b | WN (0) > 0), where P(WN (0) ≥ b) is the probability that the workload in the queue exceeds b in the stationary regime.|[25]	They also consider the so-called many-sources asymptotic framework, which is a key regime when the peak rate of sources is small compared with the server capacity and there are many-sources; other results on many-sources asymptotics can be found in [4,8,10,26].|26	When the transmission capacity C is large, and in particular, C/π = O(N), we are in the regime of the many-sources asymptotics, which have been studied by many authors, see for instance [4,10,26].|26	In this section, we extend the results and formalism developed in Likhanov and Mazumdar [26] to the continuous-time case by a discretization argument.|[26]	We now state the main result regarding the stationary tail distribution shown in [26] for the discrete-time case.|[26]	Such extensions are similarly discussed in papers by Mandjes and Kim [27] and Guibert and Simonian [17], where they assume a local convex behavior of a rate function (see Equation (3) below).|[27]	This problem has been addressed by many authors, see for instance [3,14,9,28,33,34].|28	Indeed, Massouli´e shows in [28] via a sample-path Large Deviations (sp-LD) ordering that if point processes satisfy the sp-LD principles with a finite rate function, the sp-LD dominance by the Poisson process is preserved in tandem FIFO queues.|[28]	However, Massouli´e [28] also points out that deterministic processes do not have a smaller sp-LD rate function than a Poisson process for each sample path of their multiplexing.|[28]	Another approach to studying the superposition of regulated sources in a buffer has been presented by Busson and Massouli´e [29], where they used Hoeffding’s inequality based on the fact that the total number of packet arrivals in a given time interval from a regulated source is bounded.|[29]	Now, by using a classical result by Erlang [30], we know that over the interval [jσ,(j + 1)σ] for j ≥ 0, P(Wf ≤ x) =  1 − ρ C X j i=0 (i − x/σ) i i!  ρ C i e − ρ C (i−x/σ) .|[30]	See also [31] for generalizations and alternative proofs of the results in [8,22,23].|[31]	As pointed out by the authors of [32] (see also [15]), these approaches can be regarded as applications of the Hoeffding bound given in [21] and they obtain the upper bounds via majorizing the queue length by a sum of independent processes and computing bounds based on these independent processes.|[32]	This approach was recently extended by Vojnovic and Le Boudec [32].|[32]	In this case, we can however use an upper bound for the overflow probability obtained via similar worst traffic profiles and Chernoff ’s inequality as P(WN (0) > b) ≤ e −NIt0 (Ct0+b) , (9) where the rate function It0 can be computed as in Equation (8); see also [32].|[32]	The principal difference with [32] is that in our approach, we explicitly take into account the many sources effect and determine the rate function for the source, which is extremal for the overflow asymptotics, rather than a priori first bounding the probability and then trying to make the bound small [8,32].|[32]	The principal difference with [32] is that in our approach, we explicitly take into account the many sources effect and determine the rate function for the source, which is extremal for the overflow asymptotics, rather than a priori first bounding the probability and then trying to make the bound small [8,32].|32	This problem has been addressed by many authors, see for instance [3,14,9,28,33,34].|33	This problem has been addressed by many authors, see for instance [3,14,9,28,33,34].|34	Indeed, in [34], the authors found that in a large network, each internal flow with initially fixed burst size σi can be regulated by the same pair of (σi , ρi) parameters.|[34]
b5e6da04c35a586609a46bbbd7b1ad031a658b08	At the control center, the measurement data is used for control and optimization functions, such as contingency analysis, automatic generation control, load forecasting, optimal power flow computation, and reactive power dispatch [1].|[1]	A different strategy for the detection of false data relies on statistical techniques, e.g., see [1].|[1]	The first property follows directly from [2] (cfr. page 427).|[2]	Diffusion Kalman filtering and smoothing algorithms are proposed, for instance, in [3, 5], and consensus based techniques in [25, 26].|3	Considerable effort has been devoted to the development of distributed and adaptive filtering schemes, which generalize the notion of adaptive estimation to a setup involving networked sensing and processing devices [4].|[4]	In this context, relevant methods include incremental Least Mean-Square [15], incremental Recursive Least-Square [24], Diffusive Least Mean-Square [24], and Diffusive Recursive Least-Square[4].|[4]	Diffusion Kalman filtering and smoothing algorithms are proposed, for instance, in [3, 5], and consensus based techniques in [25, 26].|5	See [13, 11, 32, 6] for a detailed discussion.|6	(Decay rate [7])|[7]	For the power system state estimation problem, several centralized and parallel solutions have been developed in the last decades, e.g., see [19, 8, 30].|8	An undirected graph is said to be connected if there exists a path between any two vertices [9].|[9]	Let H ∈ R m×n, m > n, and assume the presence of dm/ke monitors, 1 ≤ k ≤ m. Recall that, for a matrix M ∈ R k×p , the singular value decomposition can be performed with complexity O(min{kp2 , k2p}) [10].|[10]	See [13, 11, 32, 6] for a detailed discussion.|11	The references [34, 12] explore the idea of using a global control center to coordinate estimates obtained locally by several local control centers.|12	See [13, 11, 32, 6] for a detailed discussion.|13	One possibility for the attacker is to corrupt the data coming from the measuring units and directed to the control center, in order to introduce arbitrary errors in the estimated state, and, consequently, to compromise the performance of control and optimization algorithms [14].|[14]	In this context, relevant methods include incremental Least Mean-Square [15], incremental Recursive Least-Square [24], Diffusive Least Mean-Square [24], and Diffusive Recursive Least-Square[4].|[15]	A detailed comparison between incremental and diffusive methods is beyond the purpose of this work, and we refer the interested reader to [15, 16] and the references therein for a thorough discussion.|15	A detailed comparison between incremental and diffusive methods is beyond the purpose of this work, and we refer the interested reader to [15, 16] and the references therein for a thorough discussion.|16	For the equation (2), without affecting generality, assume Ker(H) = {0}, and recall from [17] that the vector xwls = (HTΣ −1H) −1HTΣ −1 z (3) minimizes the weighted variance of the estimation error, i.e., xwls = arg minxˆ (z − Hxˆ) TΣ −1 (z − Hxˆ).|[17]	It can be shown that kxˆk2 being minimum corresponds to ˆx being orthogonal to the null space Ker(H) of H [17].|[17]	Because of the increasing reliance of modern power systems on communication networks, the possibility of cyber attacks is a real threat [18].|[18]	For the power system state estimation problem, several centralized and parallel solutions have been developed in the last decades, e.g., see [19, 8, 30].|19	Moreover, even in decentralized setting, the work in [20] on the blackout of August 2003 suggests that an estimation of the entire network is essential to prevent networks damages.|[20]	For instance, our procedures can be used for state estimation and false data detection in dynamical system, as described in [21] for the case of sensors networks.|[21]	This setting, which usually requires the least amount of communications [22], induces a cyclic interaction graph among the processors.|[22]	Differently than [23], our estimation procedures assume neither the measurement error covariance nor the measurements matrix to be diagonal.|[23]	In this context, relevant methods include incremental Least Mean-Square [15], incremental Recursive Least-Square [24], Diffusive Least Mean-Square [24], and Diffusive Recursive Least-Square[4].|[24]	In this context, relevant methods include incremental Least Mean-Square [15], incremental Recursive Least-Square [24], Diffusive Least Mean-Square [24], and Diffusive Recursive Least-Square[4].|[24]	Diffusion Kalman filtering and smoothing algorithms are proposed, for instance, in [3, 5], and consensus based techniques in [25, 26].|25	Diffusion Kalman filtering and smoothing algorithms are proposed, for instance, in [3, 5], and consensus based techniques in [25, 26].|26	This concern was first recognized and addressed in [27, 28, 29] by introducing the idea of (static) state estimation in power systems.|27	This concern was first recognized and addressed in [27, 28, 29] by introducing the idea of (static) state estimation in power systems.|28	In this work, we adopt the approximated estimation model presented in [28], which follows from the linearization around the origin of equation (1).|[28]	This concern was first recognized and addressed in [27, 28, 29] by introducing the idea of (static) state estimation in power systems.|29	For the power system state estimation problem, several centralized and parallel solutions have been developed in the last decades, e.g., see [19, 8, 30].|30	An exception is constituted by [31], where a estimation technique based on local Kalman filters and a consensus strategy is developed.|[31]	See [13, 11, 32, 6] for a detailed discussion.|32	Recently, the authors of [33] show that a false data injection attack, in addition to destabilizing the grid, may also lead to fluctuations in the electricity market, causing significant economical losses.|[33]	The references [34, 12] explore the idea of using a global control center to coordinate estimates obtained locally by several local control centers.|34
45705e7cf3337e2469b5dbcaa31579c28bde89d1	In short, the techniques by Chaudhuri et al. [2011] and Bassily et al. [2014] succeed precisely because, for γ > 0, the regularized loss (3) is strongly convex.|Bassily et al. [2014]	Ghosh et al. [2014] illustrate these ideas in the context of privacy-sensitive individuals through the use of the Brier scoring rule [Brier, 1950] as a payment scheme among players holding a random bit.|[Brier, 1950]	The payment function B(q 0 , q) is the basic Brier scoring rule [Brier, 1950]; by design, it is strictly proper, i.e., it is uniquely maximized by truthful reporting. For completeness, we provide a proof.|[Brier, 1950]	Regressing a linear model over data from strategic agents that can only manipulate their costs, but not their data, was studied by Horel et al. [2014] and Cai et al. [2014], while Ioannidis and Loiseau [2013] consider a setting without payments, in which agents receive a utility as a function of estimation accuracy.|Cai et al. [2014]	In short, the techniques by Chaudhuri et al. [2011] and Bassily et al. [2014] succeed precisely because, for γ > 0, the regularized loss (3) is strongly convex.|Chaudhuri et al. [2011]	We incorporate into our mechanism the the Output Perturbation algorithm from Chaudhuri et al. [2011], which first computes the ridge regression estimator and then adds noise to the output.|Chaudhuri et al. [2011]	The following lemma follows from Chaudhuri et al. [2011]; a proof is provided for completeness.|Chaudhuri et al. [2011]	Then by Lemma 7 of Chaudhuri et al. [2011], ˆθ R − ( ˆθ R) 0 2 ≤ 4 2γ (2B + M) = 1 γ (4B + 2M).|Chaudhuri et al. [2011]	We use a model of players’ costs for privacy [Chen et al., 2013] that is based on the well-established notion of differential privacy [Dwork et al., 2006].|[Chen et al., 2013]	A related thread of work [Ghosh and Roth, 2013, Nissim et al., 2012, Chen et al., 2013] explores models of costs for privacy, based on the notion of differential privacy [Dwork et al., 2006].|Chen et al., 2013	We note that the quadratic bound in Assumption 1 was introduced by Chen et al. [2013] and also adopted by Ghosh et al. [2014]; as noted by the above authors, the quadratic bound can be shown to hold for a broad class of natural cost functions fi ; we refer the reader to Appendix D for a formal description of this class.|Chen et al. [2013]	We will consider a particular functional form of fi(ci , ), motivated by the model of privacy cost in the existing literature Chen et al. [2013].|Chen et al. [2013]	(Chen et al. [2013], Privacy Cost Assumption3 ).|Chen et al. [2013]	(Dwork et al. [2010], Chen et al. [2013], Composition Lemma).|Chen et al. [2013]	(Sketch) The first inequality comes from Lemma 5.2 of Chen et al. [2013] and plugging in our speci- fication of their “privacy-bound function” and replace statistical difference with the upper bound of e  − 1.|Chen et al. [2013]	Dekel et al. [2010] consider an analyst that regresses a “consensus” model across data coming from multiple strategic agents; agents would like the consensus value to minimize a loss over their own data, and they show that, in this setting, empirical risk minimization is group strategy-proof.|Dekel et al. [2010]	We use a model of players’ costs for privacy [Chen et al., 2013] that is based on the well-established notion of differential privacy [Dwork et al., 2006].|[Dwork et al., 2006]	A related thread of work [Ghosh and Roth, 2013, Nissim et al., 2012, Chen et al., 2013] explores models of costs for privacy, based on the notion of differential privacy [Dwork et al., 2006].|[Dwork et al., 2006]	Recall the classic definition of differential privacy by Dwork et al. [2006]:|Dwork et al. [2006]	Thus by the Composition Theorem in Dwork et al. [2006], the estimators ( ˆθ P , ˆθ P 0 , ˆθ P 1 ) together satisfy 2-differential privacy.|Dwork et al. [2006]	(Dwork et al. [2010], Chen et al. [2013], Composition Lemma).|Dwork et al. [2010]	The vast majority of this work [Fleischer and Lyu, 2012, Ligett and Roth, 2012, Nissim et al., 2014] operates in a model where agents cannot lie about their private information (their only recourse is to withhold it or perhaps to lie about their costs for privacy).|Fleischer and Lyu, 2012	This immediately poses a problem, as differentially private computation of a linear model necessarily produces a biased estimation; existing approaches [Ghosh et al., 2014] to design mechanisms to elicit data from privacy-sensitive individuals do not generalize well to biased estimators.|[Ghosh et al., 2014]	A similar effect was seen by Ghosh et al. [2014].|Ghosh et al. [2014]	Our setting is closest to, and inspired by, Ghosh et al. [2014], who bring the technology of peer prediction to bear on the problem of incentivizing truthful reporting in the presence of privacy concerns.|Ghosh et al. [2014]	Ghosh et al. [2014] adapt the peer prediction approach to overcome a number of challenges presented by privacy-sensitive individuals.|Ghosh et al. [2014]	The mechanism and analysis of Ghosh et al. [2014] was for the simplest possible statistic—the sum of a private binary type.|Ghosh et al. [2014]	Following, e.g., Kearns et al. [2014] and Ghosh et al. [2014], we depart from the classic differential privacy definition, quantifying privacy violation instead through joint differential privacy [Kearns et al., 2014].|Ghosh et al. [2014]	We note that the quadratic bound in Assumption 1 was introduced by Chen et al. [2013] and also adopted by Ghosh et al. [2014]; as noted by the above authors, the quadratic bound can be shown to hold for a broad class of natural cost functions fi ; we refer the reader to Appendix D for a formal description of this class.|Ghosh et al. [2014]	Ghosh et al. [2014] illustrate these ideas in the context of privacy-sensitive individuals through the use of the Brier scoring rule [Brier, 1950] as a payment scheme among players holding a random bit.|Ghosh et al. [2014]	Following Ghosh and Roth [2013], a series of papers have studied data acquisition problems from agents that have privacy concerns.|Ghosh and Roth [2013]	A related thread of work [Ghosh and Roth, 2013, Nissim et al., 2012, Chen et al., 2013] explores models of costs for privacy, based on the notion of differential privacy [Dwork et al., 2006].|Ghosh and Roth, 2013	This relaxation of differential privacy is natural, but it is also necessary to incentivize truthfulness [Ghosh and Roth, 2013].|[Ghosh and Roth, 2013]	As discussed in the related work section, starting from Ghosh and Roth [2013], a series of recent papers on strategic data revelation model player privacy costs as functions of the privacy parameter .|Ghosh and Roth [2013]	Regressing a linear model over data from strategic agents that can only manipulate their costs, but not their data, was studied by Horel et al. [2014] and Cai et al. [2014], while Ioannidis and Loiseau [2013] consider a setting without payments, in which agents receive a utility as a function of estimation accuracy.|Horel et al. [2014]	(Billboard Lemma [Hsu et al., 2014]).|[Hsu et al., 2014]	Regressing a linear model over data from strategic agents that can only manipulate their costs, but not their data, was studied by Horel et al. [2014] and Cai et al. [2014], while Ioannidis and Loiseau [2013] consider a setting without payments, in which agents receive a utility as a function of estimation accuracy.|Ioannidis and Loiseau [2013]	More specifically, as in Ioannidis and Loiseau [2013], we assume that a player i can manipulate her responses yi but not her features xi .|Ioannidis and Loiseau [2013]	Following, e.g., Kearns et al. [2014] and Ghosh et al. [2014], we depart from the classic differential privacy definition, quantifying privacy violation instead through joint differential privacy [Kearns et al., 2014].|Kearns et al. [2014]	Following, e.g., Kearns et al. [2014] and Ghosh et al. [2014], we depart from the classic differential privacy definition, quantifying privacy violation instead through joint differential privacy [Kearns et al., 2014].|[Kearns et al., 2014]	Then Z describes a uniform distribution over the d-dimensional unit ball Knuth [1981].|Knuth [1981]	The vast majority of this work [Fleischer and Lyu, 2012, Ligett and Roth, 2012, Nissim et al., 2014] operates in a model where agents cannot lie about their private information (their only recourse is to withhold it or perhaps to lie about their costs for privacy).|Ligett and Roth, 2012	y. Since ˆθ P 0 and ˆθ P 1 are computed on disjoint subsets of the data, then by Theorem 4 of McSherry [2009], together they satisfy -differential privacy.|McSherry [2009]	The peer prediction approach, of Miller et al. [2005], incentivizes truthful reporting (in the absence of privacy constraints) by, effectively, rewarding players for reporting information that is predictive of the reports of other agents.|Miller et al. [2005]	Peer prediction [Miller et al., 2005] is a useful method of inducing truthful reporting among players that hold data generated by the same statistical model.|[Miller et al., 2005]	[Miller et al., 2005] Under payments (2), truthful reporting is a Bayes-Nash equilibrium.|[Miller et al., 2005]	A related thread of work [Ghosh and Roth, 2013, Nissim et al., 2012, Chen et al., 2013] explores models of costs for privacy, based on the notion of differential privacy [Dwork et al., 2006].|Nissim et al., 2012	The vast majority of this work [Fleischer and Lyu, 2012, Ligett and Roth, 2012, Nissim et al., 2014] operates in a model where agents cannot lie about their private information (their only recourse is to withhold it or perhaps to lie about their costs for privacy).|Nissim et al., 2014	A similar result, albeit in a more restricted setting, is established by Perote and Perote-Pena [2004].|Perote and Perote-Pena [2004]	We address this issue for large n using again the concentration result by Vershynin [2012] (c.f. Appendix A.2).|Vershynin [2012]	The following theorem, which follows as a corollary of a result by Vershynin [2012] (see Appendix C), formalizes this notion, providing bounds on both the largest and smallest eigenvalue of X>X and γI + X>X.|Vershynin [2012]	A generalization of Theorem 7 holds for {xi}i∈[n] sampled from any distribution with a covariance Σ whose smallest eigenvalue is bounded away from zero (see Vershynin [2012]).|Vershynin [2012]	From Corollary 5.52 in Vershynin [2012] and the calculation of covariance in Lemma 11, for any ξ ∈ (0, 1) and t ≥ 1, with probability at least 1 − d −t 2 , 1 n X>X − 1 d + 2 I ≤ ξ 1 d + 2 , (7) when n ≥ C( t ξ ) 2 (d + 2) log d, for some absolute constant C.|Vershynin [2012]
ddf1ea128fbc14a203c3b3d44d0135bb4dc33ffe	The proposed framework is based on a generalization of the concepts of matrix algebra to multiple dimensions; it incorporates and unifies existing approaches from multidimensional signal processing [5,10,2] , recent developments in the field of tensor analysis [12], and multilinear algebra [27,17,16,11, 14,1,19].|1	The resulting tensor formulation helped to analyze the mathematical structure of the problem and to derive its decomposition in the form of a 1-rank tensor decomposition known as Canonical Decomposition (CANDECOMP) [11,14,1,19].|1	The properties presented above distinguish our framework from one often used in the field of tensor decompositions [27,17,16,11,14,1,19], but do not limit its use for solving problems studied in that field.|1	Modeling of fluorescence data [1]|[1]	Social Network Analysis [1]|[1]	Any arbitrary tensor can be represented as a weighted sum of decomposed tensors [11, 1, 19, 15].|1	The proposed framework is based on a generalization of the concepts of matrix algebra to multiple dimensions; it incorporates and unifies existing approaches from multidimensional signal processing [5,10,2] , recent developments in the field of tensor analysis [12], and multilinear algebra [27,17,16,11, 14,1,19].|2	Computational problems with tensor structure, which involve large amounts of multidimensional data, arise in many fields of science and engineering [3, 8, 21, 4, 18, 14].|3	Fig. 4.4 compares visually how AMG and TMG are applied to a problem of reconstruction of a twodimensional image from a few arbitrarily taken samples, using non-uniform spline interpolation [3].|[3]	Our framework was successfully applied to solve a large real-world computational problem - a spline-based global variational reconstruction [3] of a multidimensional signal from incomplete and spatially scattered measurements in four dimensions.|[3]	Our approach does not require explicit storage either of the system tensor or its decomposition, and thus the problem of size, discussed above (33’554’432 unknowns parameters), for about 9’000’000 measurements, can be computed on current inexpensive multi core computer equipped with only 2 GBytes of physical memory in less than 60 minutes, surpassing the capability of a published, matrix-based solving algorithm [3] for the same problem by far.|[3]	Least squares problems [3, 21]|3	Computational problems with tensor structure, which involve large amounts of multidimensional data, arise in many fields of science and engineering [3, 8, 21, 4, 18, 14].|4	Some aspects of such automated code generation have also been discussed in [4].|[4]	The proposed framework is based on a generalization of the concepts of matrix algebra to multiple dimensions; it incorporates and unifies existing approaches from multidimensional signal processing [5,10,2] , recent developments in the field of tensor analysis [12], and multilinear algebra [27,17,16,11, 14,1,19].|5	Vectorization of multidimensional objects may lead to loss of spatial data coherence [6,13], which can adversely affect the performance of solving algorithms.|6	Independent Component Analysis [6, 15]|6	In our framework, multidimensional data and multidimensional transformations are described based on a formalism originating in Tensor Analysis [12] and Physics [7].|[7]	For designation of the contraction, we use the Einstein convention [7] which assumes implicit summation over a pair of indices with equal designation but different variance.|[7]	Computational problems with tensor structure, which involve large amounts of multidimensional data, arise in many fields of science and engineering [3, 8, 21, 4, 18, 14].|8	Integral equations [8]|[8]	Differential equations (Poisson, Navier-Stokes, Finite Element Methods) [18, 9]|9	The proposed framework is based on a generalization of the concepts of matrix algebra to multiple dimensions; it incorporates and unifies existing approaches from multidimensional signal processing [5,10,2] , recent developments in the field of tensor analysis [12], and multilinear algebra [27,17,16,11, 14,1,19].|10	The proposed framework is based on a generalization of the concepts of matrix algebra to multiple dimensions; it incorporates and unifies existing approaches from multidimensional signal processing [5,10,2] , recent developments in the field of tensor analysis [12], and multilinear algebra [27,17,16,11, 14,1,19].|11	The resulting tensor formulation helped to analyze the mathematical structure of the problem and to derive its decomposition in the form of a 1-rank tensor decomposition known as Canonical Decomposition (CANDECOMP) [11,14,1,19].|11	The properties presented above distinguish our framework from one often used in the field of tensor decompositions [27,17,16,11,14,1,19], but do not limit its use for solving problems studied in that field.|11	Any arbitrary tensor can be represented as a weighted sum of decomposed tensors [11, 1, 19, 15].|11	The proposed framework is based on a generalization of the concepts of matrix algebra to multiple dimensions; it incorporates and unifies existing approaches from multidimensional signal processing [5,10,2] , recent developments in the field of tensor analysis [12], and multilinear algebra [27,17,16,11, 14,1,19].|[12]	In our framework, multidimensional data and multidimensional transformations are described based on a formalism originating in Tensor Analysis [12] and Physics [7].|[12]	Vectorization of multidimensional objects may lead to loss of spatial data coherence [6,13], which can adversely affect the performance of solving algorithms.|13	Data compression [13]|[13]	Computational problems with tensor structure, which involve large amounts of multidimensional data, arise in many fields of science and engineering [3, 8, 21, 4, 18, 14].|14	Introduced in Tensor Analysis and Multilinear Algebra, tensors gained the attention of practitioners of diverse fields (see the excellent review by Kolda [14]).|[14]	The proposed framework is based on a generalization of the concepts of matrix algebra to multiple dimensions; it incorporates and unifies existing approaches from multidimensional signal processing [5,10,2] , recent developments in the field of tensor analysis [12], and multilinear algebra [27,17,16,11, 14,1,19].|14	The resulting tensor formulation helped to analyze the mathematical structure of the problem and to derive its decomposition in the form of a 1-rank tensor decomposition known as Canonical Decomposition (CANDECOMP) [11,14,1,19].|14	The properties presented above distinguish our framework from one often used in the field of tensor decompositions [27,17,16,11,14,1,19], but do not limit its use for solving problems studied in that field.|14	Independent Component Analysis [6, 15]|15	Any arbitrary tensor can be represented as a weighted sum of decomposed tensors [11, 1, 19, 15].|15	The proposed framework is based on a generalization of the concepts of matrix algebra to multiple dimensions; it incorporates and unifies existing approaches from multidimensional signal processing [5,10,2] , recent developments in the field of tensor analysis [12], and multilinear algebra [27,17,16,11, 14,1,19].|16	The properties presented above distinguish our framework from one often used in the field of tensor decompositions [27,17,16,11,14,1,19], but do not limit its use for solving problems studied in that field.|16	The proposed framework is based on a generalization of the concepts of matrix algebra to multiple dimensions; it incorporates and unifies existing approaches from multidimensional signal processing [5,10,2] , recent developments in the field of tensor analysis [12], and multilinear algebra [27,17,16,11, 14,1,19].|17	The properties presented above distinguish our framework from one often used in the field of tensor decompositions [27,17,16,11,14,1,19], but do not limit its use for solving problems studied in that field.|17	This definition is compatible with, and extends [17, 21].|17	Computational problems with tensor structure, which involve large amounts of multidimensional data, arise in many fields of science and engineering [3, 8, 21, 4, 18, 14].|18	Differential equations (Poisson, Navier-Stokes, Finite Element Methods) [18, 9]|18	However, translating tensor mathematics to a convenient computational framework raises many issues [19]; including, convenient notation, ease of algorithm implementation, performance.|[19]	The proposed framework is based on a generalization of the concepts of matrix algebra to multiple dimensions; it incorporates and unifies existing approaches from multidimensional signal processing [5,10,2] , recent developments in the field of tensor analysis [12], and multilinear algebra [27,17,16,11, 14,1,19].|19	The resulting tensor formulation helped to analyze the mathematical structure of the problem and to derive its decomposition in the form of a 1-rank tensor decomposition known as Canonical Decomposition (CANDECOMP) [11,14,1,19].|19	The properties presented above distinguish our framework from one often used in the field of tensor decompositions [27,17,16,11,14,1,19], but do not limit its use for solving problems studied in that field.|19	Any arbitrary tensor can be represented as a weighted sum of decomposed tensors [11, 1, 19, 15].|19	The resulting formulation is no longer explicitly multidimensional, - a fact that may create difficulty in identifying and understanding important properties inherent to the particular problem [20].|[20]	Signal filtering [20]|[20]	Computational problems with tensor structure, which involve large amounts of multidimensional data, arise in many fields of science and engineering [3, 8, 21, 4, 18, 14].|21	Least squares problems [3, 21]|21	This definition is compatible with, and extends [17, 21].|21	Modeling of electronic and optical properties of molecules and their interactions [25, 22]|22	Another type of iterative algorithms which are frequently used for solving large inverse problems, is the family of Krylov subspace solvers, which includes GMRES, CG, BICGSTAB and other solvers [23].|23	Pattern recognition [24, 28]|24	Modeling of electronic and optical properties of molecules and their interactions [25, 22]|25	This operation is a tensor analogue of the Khatri-Rao product [26] which is a matching elementwise (over i) Kronecker product of two sets of vectors.|[26]	The proposed framework is based on a generalization of the concepts of matrix algebra to multiple dimensions; it incorporates and unifies existing approaches from multidimensional signal processing [5,10,2] , recent developments in the field of tensor analysis [12], and multilinear algebra [27,17,16,11, 14,1,19].|27	The properties presented above distinguish our framework from one often used in the field of tensor decompositions [27,17,16,11,14,1,19], but do not limit its use for solving problems studied in that field.|27	Pattern recognition [24, 28]|28
030cadedef2370bd296af07fc3324c6bb8409ba5	In space, it has been suggested that the same approach be used to define pockets in a search for casting directions 3, but in fact the difference between a polyhedron and its convex hull need not have a natural decomposition, and may have more complicated topology than the original polyhedron 1.|1	We present some basic geometric and topological concepts 2,9 in R3 that will be needed.|2	In space, it has been suggested that the same approach be used to define pockets in a search for casting directions 3, but in fact the difference between a polyhedron and its convex hull need not have a natural decomposition, and may have more complicated topology than the original polyhedron 1.|3	Edelsbrunner et al.4 defined pocketsfor α-shapes, special polyhedra that are induced by the unions of balls in three dimensions.|4	For example, cavities determine cast removal directions. Feature definitions in the CAD literature, however, are often restrictive to facilitate recognition. For example, Fields and Anderson5 define a cavity as a connected set of facets with no convex edge, which means that the tiniest bump invalidates what would otherwise be a cavity.|5	Curvature is defined at all points on a surface, and from the Gauss-Bonnet theorem 6 we know that the sum of curvature over Rfh(P) equals −4π(g−1).|6	Feature recognition is an important research area in computer-aided design and manufacturing (CAD/CAM)7,8.|7	Manufacturing features include geometric structures such as tunnels produced by drilling and cavities produced by milling or left behind by casting. Identifying these features from a solid model facilitates machining process planning or the analysis of manufacturability7,11.|7	Feature recognition is an important research area in computer-aided design and manufacturing (CAD/CAM)7,8.|8	We present some basic geometric and topological concepts 2,9 in R3 that will be needed.|9	Although some of our lemmas are general, we are primarily concerned with tame or even polyhedral sets in contrast to some of the “wild” sets defined in topology like Alexander’s horned sphere.9|9	The line hull is also known as the visual hull10 in the context of computer vision and graphics.|10	Manufacturing features include geometric structures such as tunnels produced by drilling and cavities produced by milling or left behind by casting. Identifying these features from a solid model facilitates machining process planning or the analysis of manufacturability7,11.|11	Computational geometers have identified many classes of 2D polygons (convex, starshaped, L-convex, externally visible, edge-visible, LR-visible, street, person. . . 14,16), but few classes of 3D polyhedra.|14	In the plane, the difference between a simple polygon and its convex hull is a number of simple, polygonal bays, from which one can obtain a natural description of a polygon as a tree of unions and differences of convex pieces 15.|15	Computational geometers have identified many classes of 2D polygons (convex, starshaped, L-convex, externally visible, edge-visible, LR-visible, street, person. . . 14,16), but few classes of 3D polyhedra.|16
31368c6398a34b489f78708039177d858b171d13	Existing trust models (e.g., Barber & Kim, 2001), specify how an agent may aggregate trust information from multiple sources (which could include a combination of referrals and direct interactions).|e.g., Barber & Kim, 2001	Then the posterior probability of evidence r, s is the conditional probability of x given r, s (Casella & Berger, 1990). The|Casella & Berger, 1990	Fullam and Barber (2007) study how to choose between trust from direct experience (experience-based) and from referrals (reputation-based).|Fullam and Barber 2007	For this reason, modern trust models define trust in terms of both the probability and the certainty of a good outcome (Jøsang & Ismail, 2002; Wang & Singh, 2007; G´omez, Carb´o, & Earle, 2007; Teacy, Patel, Jennings, & Luck, 2006; Harbers, Verbrugge, Sierra, & Debenham, 2007; Paradesi, Doshi, & Swaika, 2009).|G´omez, Carb´o, & Earle, 2007	Hang, Wang, and Singh (2008) proposed the Max-Certainty trust update method, which applies on Wang and Singh’s trust representation.|Hang, Wang, and Singh 2008	For this reason, modern trust models define trust in terms of both the probability and the certainty of a good outcome (Jøsang & Ismail, 2002; Wang & Singh, 2007; G´omez, Carb´o, & Earle, 2007; Teacy, Patel, Jennings, & Luck, 2006; Harbers, Verbrugge, Sierra, & Debenham, 2007; Paradesi, Doshi, & Swaika, 2009).|Harbers, Verbrugge, Sierra, & Debenham, 2007	Hazard (2010) has studied such dynamical properties of various mechanisms but without explicitly considering referrals.|Hazard 2010	Hazard and Singh (2010) identify and axiomatize some common intuitions about trust when viewed from the perspective of the incentives of agents.|Hazard and Singh 2010	Besides, to reflect the dynamism of agents over time, a discount factor is needed to help trust models provide accurate predictions of future behavior (Zacharia & Maes, 2000; Huynh, Jennings, & Shadbolt, 2006).|Huynh, Jennings, & Shadbolt, 2006	FIRE (Huynh et al., 2006) and REGRET (Sabater & Sierra, 2002) are two trust models that consider trust information from both individual and social aspects.|Huynh et al., 2006	Wang and Singh (2006) provide a concatenation operator (similar to Jøsang’s, 1998, recommendation operator) that enables a client C to compute how much trust it should place in a service provider S based on its direct experience with a referrer R and a referral for S provided by R.|Jøsang’s, 1998	Jøsang (1998) defines certainty as r +s r+s+1 , which yields little difference from a later work by Jøsang (2001) in trust update. Our|Jøsang 1998	Following Jøsang (2001), we interpret the above as a probability of a probability or a probability-certainty distribution function (PCDF).|Jøsang 2001	Wang and Singh (2007) differ from Jøsang (2001) in their definition of certainty.|Jøsang 2001	To handle the situation where C collects trust information of S from more than one source, we use Jøsang’s aggregation operator (Jøsang, 2001; Wang & Singh, 2007), which simply sums the available evidence pro and con.|Jøsang, 2001	Linear-WS, as shown in Algorithm 2, adopts Wang and Singh’s notion of certainty underlying trust (Definition 1), whereas Algorithm 3 depicts Jøsang trust update method, where the certainty c is defined as r +s r+s+2 (Jøsang, 2001).|Jøsang 2001	Jøsang (1998) defines certainty as r +s r+s+1 , which yields little difference from a later work by Jøsang (2001) in trust update. Our|Jøsang 2001	For this reason, modern trust models define trust in terms of both the probability and the certainty of a good outcome (Jøsang & Ismail, 2002; Wang & Singh, 2007; G´omez, Carb´o, & Earle, 2007; Teacy, Patel, Jennings, & Luck, 2006; Harbers, Verbrugge, Sierra, & Debenham, 2007; Paradesi, Doshi, & Swaika, 2009).|Jøsang & Ismail, 2002	The Beta Reputation System (BRS) (Jøsang & Ismail, 2002) and SPORAS (Zacharia & Maes, 2000) are two trust models that support the idea of the discount factor.|Jøsang & Ismail, 2002	The reputation of each child reflects the new evidence (the error) based on two update schemes, β-reputation (Jøsang & Ismail, 2002) and Q-learning (Watkins & Dayan, 1992).|Jøsang & Ismail, 2002	Trust models have been widely studied (Sabater & Sierra, 2005; Jøsang, Ismail, & Boyd, 2007).|Jøsang, Ismail, & Boyd, 2007	Also, from the incentives perspective, Jurca and Faltings (2007) study mechanisms to ensure agents offer truthful feedback on others.|Jurca and Faltings 2007	Khosravifar, Gomrokchi, and Bentahar (2009) design a maintenance-based trust model.|Khosravifar, Gomrokchi, and Bentahar 2009	Mistry, G¨ursel, and Sen (2009) estimate reputation scores of sensor nodes based on measurement accuracy in sensor networks.|Mistry, G¨ursel, and Sen 2009	For this reason, modern trust models define trust in terms of both the probability and the certainty of a good outcome (Jøsang & Ismail, 2002; Wang & Singh, 2007; G´omez, Carb´o, & Earle, 2007; Teacy, Patel, Jennings, & Luck, 2006; Harbers, Verbrugge, Sierra, & Debenham, 2007; Paradesi, Doshi, & Swaika, 2009).|Paradesi, Doshi, & Swaika, 2009	Paradesi et al. (2009) incorporate trust based on certainty into their work on web service composition.|Paradesi et al. 2009	Referral networks enable agents to collect trust information about service providers in a distributed manner (Yu & Singh, 2002; Procaccia, Bachrach, & Rosenschein, 2007).|Procaccia, Bachrach, & Rosenschein, 2007	Ries and Heinemann (2008) propose CertainTrust, which is similar to Jøsang’s approach.|Ries and Heinemann 2008	FIRE (Huynh et al., 2006) and REGRET (Sabater & Sierra, 2002) are two trust models that consider trust information from both individual and social aspects.|Sabater & Sierra, 2002	Trust models have been widely studied (Sabater & Sierra, 2005; Jøsang, Ismail, & Boyd, 2007).|Sabater & Sierra, 2005	Poyraz (Sensoy, Zhang, Yolum, & Cohen, 2009) is a trust-based service selection approach.|Sensoy, Zhang, Yolum, & Cohen, 2009	They relate the trustworthiness of an agent to how it discounts future payoffs: more trustworthy agents have a longer time horizon, an intuition also shared by Smith and desJardins (2009).|Smith and desJardins 2009	For this reason, modern trust models define trust in terms of both the probability and the certainty of a good outcome (Jøsang & Ismail, 2002; Wang & Singh, 2007; G´omez, Carb´o, & Earle, 2007; Teacy, Patel, Jennings, & Luck, 2006; Harbers, Verbrugge, Sierra, & Debenham, 2007; Paradesi, Doshi, & Swaika, 2009).|Teacy, Patel, Jennings, & Luck, 2006	Teacy et al. (2006) develop Travos, one of the trust models based on the beta distribution.|Teacy et al. 2006	Vogiatzis, MacGillivray, and Chli (2010) build a trust framework on Hidden Markov Models.|Vogiatzis, MacGillivray, and Chli 2010	Vogiatzis et al. (2010) evaluate their approach with respect to two types of behaviors: static and damping.|Vogiatzis et al. 2010	Wang and Vassileva (2003) present a Bayesian network-based trust and reputation model for peer-to-peer networks.|Wang and Vassileva 2003	Wang and Singh (2006) define mathematical operators for propagating trust.|Wang and Singh 2006	In our chosen framework, Wang and Singh (2006) define mathematical operators for propagating trust, which we can leverage for our present goals|Wang and Singh 2006	Wang and Singh (2006) provide a concatenation operator (similar to Jøsang’s, 1998, recommendation operator) that enables a client C to compute how much trust it should place in a service provider S based on its direct experience with a referrer R and a referral for S provided by R.|Wang and Singh 2006	For this reason, modern trust models define trust in terms of both the probability and the certainty of a good outcome (Jøsang & Ismail, 2002; Wang & Singh, 2007; G´omez, Carb´o, & Earle, 2007; Teacy, Patel, Jennings, & Luck, 2006; Harbers, Verbrugge, Sierra, & Debenham, 2007; Paradesi, Doshi, & Swaika, 2009).|Wang & Singh, 2007	Wang and Singh (2007) differ from Jøsang (2001) in their definition of certainty.|Wang and Singh 2007	This section introduces the key background on Wang and Singh’s (2007) approach that is necessary for understanding our present contribution.|Wang and Singh’s 2007	For the above reason, Wang and Singh (2007) define certainty to be the area above the uniform distribution f(x) = 1.|Wang and Singh 2007	Wang and Singh (2007) prove that certainty increases when the total number of transactions increases and the conflict is fixed, as in Figure 2.|Wang and Singh 2007	To handle the situation where C collects trust information of S from more than one source, we use Jøsang’s aggregation operator (Jøsang, 2001; Wang & Singh, 2007), which simply sums the available evidence pro and con.|Wang & Singh, 2007	Like our approach, their definition of trust and certainty is based on Wang and Singh’s (2007) approach.|Wang and Singh’s 2007	In general, the certainty of a trust value should (a) increase as the amount of information increases with a fixed probability, and (b) decrease as the number of conflicts increases with a fixed total number of experiences (Wang & Singh, 2010).|Wang & Singh, 2010	We propose a probabilistic approach for updating trust that builds on Wang and Singh’s (2010) probability-certainty trust model.|Wang and Singh’s 2010	The reputation of each child reflects the new evidence (the error) based on two update schemes, β-reputation (Jøsang & Ismail, 2002) and Q-learning (Watkins & Dayan, 1992).|Watkins & Dayan, 1992	The parent compares the aggregated report and sensed data from each child, and then calculates the error based on the Wilcoxon Signed Rank Test (Wilcoxon, 1945).|Wilcoxon, 1945	Referral networks enable agents to collect trust information about service providers in a distributed manner (Yu & Singh, 2002; Procaccia, Bachrach, & Rosenschein, 2007).|Yu & Singh, 2002	Besides, to reflect the dynamism of agents over time, a discount factor is needed to help trust models provide accurate predictions of future behavior (Zacharia & Maes, 2000; Huynh, Jennings, & Shadbolt, 2006).|Zacharia & Maes, 2000	In addition, we discount each past transaction by its age (Zacharia & Maes, 2000; ?, ?, ?, ?, ?).|Zacharia & Maes, 2000	The Beta Reputation System (BRS) (Jøsang & Ismail, 2002) and SPORAS (Zacharia & Maes, 2000) are two trust models that support the idea of the discount factor.|Zacharia & Maes, 2000
97a6613da7eeb0ac4f70ae2e3b793364c794e6dc	For surveys of image registration including nonlinear medical image registration, see [20, 13, 12, 8, 18, 1, 14].|1	Except for a few studies [15, 19, 16, 10], most of the elastic deformations based nonrigid registrations rely on the assumption that image intensities remain constant between images [2, 3, 12, 18], which is not always true and affects the accuracy of motion and deformations obtained from the transformation.|2	Except for a few studies [15, 19, 16, 10], most of the elastic deformations based nonrigid registrations rely on the assumption that image intensities remain constant between images [2, 3, 12, 18], which is not always true and affects the accuracy of motion and deformations obtained from the transformation.|3	The standard intensity scale is obtained by a standardisation procedure which corrects image intensity variations [4].|[4]	Since MR image intensities do not have a fixed tissue meaning in image scale even within the same protocol, for the same body region, for images obtained on the same scanner, for the same patient, there is a strong need to transform image scale into standard intensity scale in order so that for the same MR protocol and body region, similar intensities will have similar tissue meaning [4].|[4]	Based on the experiments in [11, 4], minimum and maximum percentile values are set to pc1 = 0 and pc2 = 99.8 respectively.|4	For 2D image registration, an affine transformation has six parameters, which can be determined if the coordinates of at least three non-colinear corresponding points in the images are known [5].|[5]	More control points may improve the registration accuracy, but the computation time will also increase dramatically [6].|[6]	Multilevel continuation is well established for optimization problems and systems of non-linear equations [7].|[7]	For surveys of image registration including nonlinear medical image registration, see [20, 13, 12, 8, 18, 1, 14].|8	In rigid registration, the recovered transformation itself has no clinical significance, however, in nonrigid registration the recovered transformation may have clinical significance [8].|[8]	Since the velocity field at each image point has two components while the changes in image brightness at a point in the image plane due to motion yields only one constraint, the optical flow cannot be computed at a point in the image independently of neighboring points without introducing additional constraints [9].|[9]	Except for a few studies [15, 19, 16, 10], most of the elastic deformations based nonrigid registrations rely on the assumption that image intensities remain constant between images [2, 3, 12, 18], which is not always true and affects the accuracy of motion and deformations obtained from the transformation.|10	In [10], a fast elastic multidimensional intensity-based image registration with a parametric model of deformation is presented.|[10]	Based on the study [11], image histograms of the same body region are always of the same type.|[11]	Since most of the protocols studied in [11] produce bimodal histograms, bimodal histogram distribution is used to extract histogram specific landmarks.|[11]	The formula for mapping x ∈ [p1j , p2j ] to x ′ is the following [11].|[11]	Further details can be found in [11].|[11]	Based on the experiments in [11, 4], minimum and maximum percentile values are set to pc1 = 0 and pc2 = 99.8 respectively.|11	For surveys of image registration including nonlinear medical image registration, see [20, 13, 12, 8, 18, 1, 14].|12	Except for a few studies [15, 19, 16, 10], most of the elastic deformations based nonrigid registrations rely on the assumption that image intensities remain constant between images [2, 3, 12, 18], which is not always true and affects the accuracy of motion and deformations obtained from the transformation.|12	For surveys of image registration including nonlinear medical image registration, see [20, 13, 12, 8, 18, 1, 14].|13	For surveys of image registration including nonlinear medical image registration, see [20, 13, 12, 8, 18, 1, 14].|14	Except for a few studies [15, 19, 16, 10], most of the elastic deformations based nonrigid registrations rely on the assumption that image intensities remain constant between images [2, 3, 12, 18], which is not always true and affects the accuracy of motion and deformations obtained from the transformation.|15	To address this problem, a locally affine but globally smooth transformation model has been developed in the presence of intensity variations in [15].|[15]	Except for a few studies [15, 19, 16, 10], most of the elastic deformations based nonrigid registrations rely on the assumption that image intensities remain constant between images [2, 3, 12, 18], which is not always true and affects the accuracy of motion and deformations obtained from the transformation.|16	In [16], voxel based similarity measures, such as normalized mutual information, are combined with B-spline based nonrigid transformation called free-form deformation (FFD).|[16]	Moreover, it is likely that the criterion to optimize has a reduced number of local optima; this is due to a loss of image details and results in enhanced robustness [17].|[17]	To get optimal starting conditions, it is crucial that the coarse levels of the pyramid most represent the finest level [17].|[17]	For surveys of image registration including nonlinear medical image registration, see [20, 13, 12, 8, 18, 1, 14].|18	Except for a few studies [15, 19, 16, 10], most of the elastic deformations based nonrigid registrations rely on the assumption that image intensities remain constant between images [2, 3, 12, 18], which is not always true and affects the accuracy of motion and deformations obtained from the transformation.|18	Except for a few studies [15, 19, 16, 10], most of the elastic deformations based nonrigid registrations rely on the assumption that image intensities remain constant between images [2, 3, 12, 18], which is not always true and affects the accuracy of motion and deformations obtained from the transformation.|19	In order to remove inef- ficiency and inaccuracy arising from certain circumstances, such as low-resolution images, Bayesian based importance sampling technique with the same spatially varying parameters are used in [19].|[19]	For surveys of image registration including nonlinear medical image registration, see [20, 13, 12, 8, 18, 1, 14].|20
41b3a5272d0c8f98b73e7275481cd917802ad8b7	To that end, natural performance measures to consider would be bijective and average analysis [1].|[1]	The analysis of problems and algorithms for streaming applications, treating them as online problems, was started in [2].|[2]	An online streaming problem was first studied from an online algorithms perspective using competitive analysis by Becchetti and Koutsoupias [2].|[2]	Another natural extension of this work is to consider multiple buffers, which also allows for a richer collection of algorithms [3], or more complicated, not necessarily discrete, objective functions [10].|[3]	We analyze the frequent items problem using relative interval analysis [13] and relative worst order analysis [4].|[4]	Relative worst order analysis [4] compares two online algorithms directly.|[4]	The definition of this measure is somewhat more involved; see [5] for more intuition on the various elements.|[5]	A more comprehensive study of a larger number of performance measures on the same problem scenarios was initiated in [8] and this line of work has been continued in [9, 6, 7].|6	A more comprehensive study of a larger number of performance measures on the same problem scenarios was initiated in [8] and this line of work has been continued in [9, 6, 7].|7	A more comprehensive study of a larger number of performance measures on the same problem scenarios was initiated in [8] and this line of work has been continued in [9, 6, 7].|[8]	A more comprehensive study of a larger number of performance measures on the same problem scenarios was initiated in [8] and this line of work has been continued in [9, 6, 7].|9	Another natural extension of this work is to consider multiple buffers, which also allows for a richer collection of algorithms [3], or more complicated, not necessarily discrete, objective functions [10].|[10]	In the frequent items problem [11], an algorithm must store an item, or more generally a number of items, in a buffer, and the objective is to store the items appearing most frequently in the entire stream.|[11]	Functions have also been considered in [12].|[12]	We analyze the frequent items problem using relative interval analysis [13] and relative worst order analysis [4].|[13]	Dorrigiv et al. [13] proposed another analysis method, relative interval analysis, in the context of paging.|[13]	Here we define this analysis for maximization problems for two algorithms A and B, following [13].|[13]	This problem has been studied in [14].|[14]	In addition, we tighten the competitive analysis [16, 15] results from [14].|[14]	We compare the quality of the achieved aggregate frequencies of three different deterministic online algorithms from [14]: the naive algorithm (Nai), the eager algorithm (Eag), and the majority algorithm (Maj).|[14]	Giannakopoulos et al. has proved that no randomized algorithm for the online frequent items problem, where the buffer has room for one item, can have a competitive function better than 1 3 √ n [14].|[14]	In [14], Giannakopoulos et al. proved that for all sequences I of length n, Opt(I) ≤ √ n · Nai(I).|[14]	For Maj Giannakopoulos et al. [14] proved a competitive ratio of Θ(n).|[14]	In addition, we tighten the competitive analysis [16, 15] results from [14].|15	Competitive analysis[16, 15] evaluates an online algorithm in comparison to an optimal offline algorithm.|15	In addition, we tighten the competitive analysis [16, 15] results from [14].|16	It has been known since the start of the area that competitive analysis does not always give good results [16] and many alternatives have been proposed.|[16]	Competitive analysis[16, 15] evaluates an online algorithm in comparison to an optimal offline algorithm.|16
5268d3d7f15ffa9c6a904a138b2b2794263c856e	Arora, Bollob´as and Lov´asz [1] studied the effect of adding odd-cycle inequalities to the LP relaxation of the VCP.|[1]	They proposed that the integrality gap of the LP with all the odd-cycle inequalities is basically 2 in [1].|[1]	It seems that the ELP Algorithm may not guarantee a 3 2 -approximate solution for VCP on all graphs, since it is based on the optimal objective function value of the ELP relaxation and the integrality gap of the ELP is basically 2 [1].|[1]	The proof in [1] is probabilistic in nature and establishes existence of a graph for which integrality gap is 2.|[1]	Recently Asgeirsson and Stein [2, 3] reported extensive experimental results using a heuristic algorithm which obtained no worse than 3 2 -approximate solutions for all the test problems they considered.|2	This reduction was considered earlier by many researchers including most recently by Asgeirsson and Stein [2, 3].|2	Recently Asgeirsson and Stein [2, 3] reported extensive experimental results using a heuristic algorithm which obtained no worse than 3 2 -approximate solutions for all the test problems they considered.|3	This reduction was considered earlier by many researchers including most recently by Asgeirsson and Stein [2, 3].|3	The current best known bound on the performance ratio of a polynomial time approximation algorithm for VCP is 2 − Θ( √ 1 log n ) [12], which improved the previously known ratio of 2 − log log n 2 log n [4, 17].|4	Other SDP-relaxations of the VCP were studied in [5, 15].|5	It is known that computing a δ- approximate solution in polynomial time for VCP is NP-Hard for any δ ≤ 10√ 5−21 ' 1.36 [6], which improved the previously known non-approximability bound of 7 6 in [9].|[6]	Halperin [7] showed that an approximation ratio of 2− 2 log log ∆ log ∆ can be obtained with the semidefinite programming (SDP) relaxation of VCP where ∆ is the maximum degree of G.|[7]	Under the assumption of unique game conjecture [8, 13, 14] many researchers believe that a polynomial time 2 −  approximation algorithm is not possible for VCP.|8	It is known that computing a δ- approximate solution in polynomial time for VCP is NP-Hard for any δ ≤ 10√ 5−21 ' 1.36 [6], which improved the previously known non-approximability bound of 7 6 in [9].|[9]	There has been considerable work (see e.g. survey paper [11]) on the problem over the past 30 years on finding a polynomial-time approximation algorithm with an improved performance guarantee.|[11]	The current best known bound on the performance ratio of a polynomial time approximation algorithm for VCP is 2 − Θ( √ 1 log n ) [12], which improved the previously known ratio of 2 − log log n 2 log n [4, 17].|[12]	Under the assumption of unique game conjecture [8, 13, 14] many researchers believe that a polynomial time 2 −  approximation algorithm is not possible for VCP.|13	Under the assumption of unique game conjecture [8, 13, 14] many researchers believe that a polynomial time 2 −  approximation algorithm is not possible for VCP.|14	Other SDP-relaxations of the VCP were studied in [5, 15].|15	The current best known bound on the performance ratio of a polynomial time approximation algorithm for VCP is 2 − Θ( √ 1 log n ) [12], which improved the previously known ratio of 2 − log log n 2 log n [4, 17].|17	It is well known that an optimal vertex cover of a graph can be approximated within a factor of 2 in polynomial time by taking all the vertices of a maximal (not necessarily maximum) matching in the graph or rounding the LP relaxation solution of an integer programming formulation [18].|[18]
